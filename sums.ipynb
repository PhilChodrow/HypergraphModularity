{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "using StatsBase\n",
    "using Combinatorics\n",
    "include(\"jl/vol.jl\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# toy data\n",
    "\n",
    "Z = [1, 1, 2, 2, 3, 3, 4, 4, 4, 5, 5] # group partition\n",
    "D = [3, 4, 2, 5, 6, 4, 3, 2, 5, 2, 2]; # degree sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First: let's check the formula\n",
    "\n",
    "$$\\sum_{R \\in [n]^\\ell} \\mathbb{I}(\\#(\\mathbf{z}_R) = p) \\sigma(\\theta_R) = \\sum_{y: \\#y = p}\\prod_{a = 1}^{\\ell}v_{y_a}\\;,$$\n",
    "\n",
    "In this formula, we're allowing R to range over $[n]^\\ell$, where $\\ell$ is the number of nodes per hyperedge, and $n$ the number of nodes.  $p$ is a specified permutation. This is **different** from the current version of the nodes, and matches the conversation we had over email. \n",
    "\n",
    "The function `evalSumNaive` performs the summation on the lefthand side, while the function `evalSumPV` uses the product-of-volumes representation on the righthand side. Both of these functions are defined in `jl/sums.jl`.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "true\n",
      "true\n",
      "true\n",
      "true\n",
      "true\n",
      "true\n"
     ]
    }
   ],
   "source": [
    "# test: check that these two functions give the same result on all partitions\n",
    "# this will be very slow for even moderate n and r\n",
    "\n",
    "r = 3\n",
    "\n",
    "for i = 1:r, j = 1:i, p in partitions(i,j)\n",
    "    println(evalSumNaive(p, Z, D) == evalSumPV(p, Z, D))\n",
    "end"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, looks good! `evalSumPV` is a lot faster than `evalSumNaive`, although they are both very slow. Presumably this is due in part to kludgy coding on my part, but one would imagine that even much better coding practice could only improve these so much"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0.094113 seconds (30.42 k allocations: 94.904 MiB, 27.77% gc time)\n",
      "  0.004917 seconds (2.65 k allocations: 8.912 MiB)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "27546"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "p = [2, 1]\n",
    "@time evalSumNaive(p, Z, D)\n",
    "@time evalSumPV(p, Z, D)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can use `evalSumsPV` to compute the required sum for each partition. In principle, one could do this with `evalSumsNaive` as well (also implemented), but the latter is VERY slow. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1.294968 seconds (504.27 k allocations: 1.791 GiB, 26.14% gc time)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Dict{Any,Any} with 18 entries:\n",
       "  [2, 2, 1]       => 23050800\n",
       "  [1, 1, 1, 1]    => 346080\n",
       "  [3, 2]          => 6288620\n",
       "  [2, 2]          => 220614\n",
       "  [3]             => 2750\n",
       "  [1, 1]          => 1130\n",
       "  [1, 1, 1]       => 24576\n",
       "  [2]             => 314\n",
       "  [2, 1, 1]       => 1175616\n",
       "  [2, 1]          => 27546\n",
       "  [4, 1]          => 3587830\n",
       "  [4]             => 25058\n",
       "  [1]             => 38\n",
       "  [5]             => 234638\n",
       "  [2, 1, 1, 1]    => 26997600\n",
       "  [3, 1, 1]       => 16723680\n",
       "  [1, 1, 1, 1, 1] => 2352000\n",
       "  [3, 1]          => 317768"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# need to run twice to avoid timing compile times\n",
    "r = 5 # size of largest hyperedge\n",
    "@time MPV = evalSumsPV(Z, D, r)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remembering that n = 10 in this case and that we will need to evaluate these sums many many times, this timing is not practical. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we'll use the `evalSums`, which uses a recursion lemma from the notes in order to compute the relevant sums for every partition vector p at once. Rather than a simple loop over all partitions like `evalSumsPV` uses, `evalSums` actually uses previously calculated values in order to significantly reduce the per-partition compute time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0.000745 seconds (2.00 k allocations: 799.281 KiB)\n"
     ]
    }
   ],
   "source": [
    "# need to run this block twice in order to avoid timing compile time\n",
    "@time V, μ, M = evalSums(Z, D, r);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The result agrees with `evalSumsPV` on all partitions, but is roughly 10,000 times as fast. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# How big can we go?\n",
    "\n",
    "Even with my highly non-optimized code, we can do medium-sized instances with large hyperedges fairly quickly this way. Note, however, that we need BigInts to avoid overflow issues. \n",
    "\n",
    "With this method, we can compute on n = 50,000 nodes and hyperedges of up to size 20 in roughly the same compute time that it took `evalSumsPV` to do 10 nodes and hyperedges up to size 5, before further optimization. In a later test (not shown), I was able to compute on 1M nodes in under a minute. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  1.405072 seconds (10.27 M allocations: 222.936 MiB, 22.34% gc time)\n"
     ]
    }
   ],
   "source": [
    "# Performance test: how big can we do this?\n",
    "n = 50000\n",
    "\n",
    "Z = rand(1:50, n)\n",
    "D = rand(2:100, n)\n",
    "\n",
    "r = 10 # maximum hyperedge size\n",
    "\n",
    "@time V, μ, M = evalSums(Z, D, r;constants=true, bigInt=true);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculating Local Updates\n",
    "\n",
    "We'd now like to see if we can use this method to compute the change in the entries of M associated with moving node $i$ from group $s$ to group $t$. In principle, all we need to do is a bit of bookkeeping that will allow us to propagate this change through the entries of $M$ in a fast way. This could potentially be a foundation for a Louvain-style approach, in which we would then need to evaluate the total impact of a move and choose from among a set of possibilities to find the best one. \n",
    "\n",
    "Let's first go back to our toy data: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# toy data\n",
    "\n",
    "Z = [1, 1, 2, 2, 3, 3, 4, 4, 4, 5, 5] # group partition\n",
    "D = [3, 4, 2, 5, 6, 4, 3, 2, 5, 2, 2]; # degree sequence\n",
    "\n",
    "r = 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's a function that we'll use to test our updates. It first calculates N for the specified partition set, and swaps nodes between groups randomly and evaluates the change in $N$ associated with each swap, for a specified number of rounds. Choosing `check=true` has it compare the result to the ground-truth sums N computed on the modified grouping vector. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "function testUpdates(Z, D, r, rounds=100, check=true, bigInt=false)\n",
    "    \n",
    "    ℓ = maximum(Z) # number of total groups\n",
    "    \n",
    "    V, μ, M = evalSums(Z, D, r;constants=false, bigInt=bigInt);\n",
    "    C = evalConstants(r)\n",
    "\n",
    "    N = Dict()\n",
    "\n",
    "    for k in 1:rounds\n",
    "\n",
    "        # random proposal swap\n",
    "        i = rand(1:length(D)) # node to move\n",
    "        t = rand(1:ℓ) # new group\n",
    "        s = Z[i] # old group\n",
    "\n",
    "        # increments due to proposal\n",
    "        ΔV, Δμ, ΔM = increments(V, μ, M, i, t, D, Z);\n",
    "\n",
    "        # new quantities (assumes we accept every proposal)\n",
    "        V, μ, M = addIncrements(V, μ, M, ΔV, Δμ, ΔM)\n",
    "\n",
    "        # carry out the change in membership\n",
    "        Z[i] = t\n",
    "\n",
    "        # multiply by combinatorial factors to get the sums we actually want\n",
    "        N = Dict(p => M[p]*C[p] for p in keys(M))\n",
    "    end\n",
    "    if check\n",
    "        V̄, μ̄, N̄ = evalSums(Z, D, r; constants=true)\n",
    "        Dict(p => N[p] == N[p] for p in keys(M))\n",
    "    end     \n",
    "end;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's choose `check=true`, so we can compare to $N$ as computed from scratch. We are looking for these to agree on all partitions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dict{Array{Integer,1},Bool} with 18 entries:\n",
       "  Integer[2, 2, 1]       => true\n",
       "  Integer[1, 1, 1, 1]    => true\n",
       "  Integer[3, 2]          => true\n",
       "  Integer[2, 2]          => true\n",
       "  Integer[3]             => true\n",
       "  Integer[1, 1]          => true\n",
       "  Integer[1, 1, 1]       => true\n",
       "  Integer[2]             => true\n",
       "  Integer[2, 1, 1]       => true\n",
       "  Integer[2, 1]          => true\n",
       "  Integer[4, 1]          => true\n",
       "  Integer[4]             => true\n",
       "  Integer[1]             => true\n",
       "  Integer[5]             => true\n",
       "  Integer[2, 1, 1, 1]    => true\n",
       "  Integer[3, 1, 1]       => true\n",
       "  Integer[1, 1, 1, 1, 1] => true\n",
       "  Integer[3, 1]          => true"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testUpdates(Z, D, 5, 100, true, true)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks ok! Now let's try to get a sense for how we can do on a larger instance. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "n = 50000\n",
    "Z = rand(1:50, n)\n",
    "D = rand(2:100, n)\n",
    "r = 10 # maximum hyperedge size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  4.145152 seconds (30.72 M allocations: 903.154 MiB, 16.28% gc time)\n"
     ]
    }
   ],
   "source": [
    "n_steps = 1000\n",
    "@time testUpdates(Z, D, r, n_steps, false, true);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that, on an instance of this size, it took us ~1.5s to compute all the required sums from scratch. On the other hand, we can maintain bookkeeping through 1000 updates for only 3s more, which is a pretty good computational savings. I'm pretty convinced that these results can be improved considerably by better coding. It's also worth noting that the size of the hyperedge is responsible for much of the scaling behavior: reducing `r` significantly reduces the runtime. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.4.1",
   "language": "julia",
   "name": "julia-1.4"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.4.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
